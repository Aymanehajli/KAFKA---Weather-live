# Exercice 10 : DÃ©tection des records climatiques locaux

## Description

Cet exercice consiste Ã  crÃ©er un job Spark qui analyse les donnÃ©es historiques stockÃ©es dans HDFS, calcule les records climatiques pour chaque ville, et Ã©met ces records dans Kafka et HDFS pour exploitation par un dashboard.

## PrÃ©requis

1. **Services Docker dÃ©marrÃ©s** (Kafka, HDFS, Spark)
2. **DonnÃ©es historiques dans HDFS** (voir Exercice 9)
3. **Topic Kafka `weather_records`** (crÃ©Ã© automatiquement par le script)

## Records calculÃ©s

Pour chaque ville, le job calcule :

### 1. Jour le plus chaud de la dÃ©cennie
- **Date** du jour avec la tempÃ©rature maximale la plus Ã©levÃ©e
- **TempÃ©rature** maximale enregistrÃ©e

### 2. Jour le plus froid de la dÃ©cennie
- **Date** du jour avec la tempÃ©rature minimale la plus basse
- **TempÃ©rature** minimale enregistrÃ©e

### 3. Rafale de vent la plus forte
- **Date** du jour avec la vitesse de vent maximale la plus Ã©levÃ©e
- **Vitesse du vent** maximale enregistrÃ©e (m/s)

### 4. Jour le plus pluvieux
- **Date** du jour avec les prÃ©cipitations les plus importantes
- **PrÃ©cipitations** totales enregistrÃ©es (mm)

## Utilisation

### Option 1 : Script automatique (recommandÃ©)

```bash
./run_records_analyzer.sh
```

### Option 2 : ExÃ©cution manuelle

```bash
# 1. Copier le script dans le conteneur
docker cp weather_records_analyzer.py pyspark_notebook:/home/jovyan/work/

# 2. ExÃ©cuter avec spark-submit
docker exec -it pyspark_notebook \
  /usr/local/spark/bin/spark-submit \
  --master local[*] \
  /home/jovyan/work/weather_records_analyzer.py \
  --hdfs-path /hdfs-data \
  --kafka-servers kafka:9092 \
  --kafka-topic weather_records
```

## Options disponibles

- `--hdfs-path` : Chemin HDFS de base (dÃ©faut: `/hdfs-data`)
- `--kafka-servers` : Serveurs Kafka bootstrap (dÃ©faut: `localhost:29092`)
- `--kafka-topic` : Topic Kafka (dÃ©faut: `weather_records`)
- `--skip-kafka` : Ne pas envoyer Ã  Kafka, seulement HDFS
- `--skip-hdfs` : Ne pas sauvegarder dans HDFS, seulement Kafka

## Structure HDFS des records

Les records sont sauvegardÃ©s dans HDFS avec la structure suivante :

```
/hdfs-data/
  â”œâ”€â”€ France/
  â”‚   â””â”€â”€ Paris/
  â”‚       â””â”€â”€ weather_records/
  â”‚           â””â”€â”€ records.json
  â”œâ”€â”€ USA/
  â”‚   â””â”€â”€ New_York/
  â”‚       â””â”€â”€ weather_records/
  â”‚           â””â”€â”€ records.json
  â””â”€â”€ UK/
      â””â”€â”€ London/
          â””â”€â”€ weather_records/
              â””â”€â”€ records.json
```

## Format des records

### Structure JSON sauvegardÃ©e

```json
{
  "city": "Paris",
  "country": "France",
  "latitude": 48.8566,
  "longitude": 2.3522,
  "records": {
    "hottest_day": {
      "date": "2019-07-25",
      "temperature": 42.6
    },
    "coldest_day": {
      "date": "2018-02-28",
      "temperature": -10.2
    },
    "strongest_wind": {
      "date": "2020-12-10",
      "windspeed": 28.5
    },
    "rainiest_day": {
      "date": "2016-06-01",
      "precipitation": 45.8
    }
  },
  "statistics": {
    "total_days": 3650
  }
}
```

### Format Kafka

Les records envoyÃ©s Ã  Kafka ont le mÃªme format, avec en plus :
- `computed_at` : Timestamp de calcul du record

## Test complet

### 1. PrÃ©parer les donnÃ©es historiques

```bash
# TÃ©lÃ©charger 10 ans de donnÃ©es pour Paris
python3 weather_history_loader.py --city Paris --country France --years 10

# TÃ©lÃ©charger pour d'autres villes si nÃ©cessaire
python3 weather_history_loader.py --city "New York" --country USA --years 10
```

### 2. ExÃ©cuter l'analyseur

```bash
./run_records_analyzer.sh
```

### 3. VÃ©rifier les records dans HDFS

```bash
# Lister la structure
docker exec namenode hdfs dfs -ls -R /hdfs-data/France/Paris/

# Afficher les records
docker exec namenode hdfs dfs -cat /hdfs-data/France/Paris/weather_records/records.json
```

### 4. VÃ©rifier les records dans Kafka

```bash
# Consommer depuis le topic
python3 kafka_consumer.py weather_records --from-beginning
```

## Exemple de sortie

```
==========================================
ðŸ“Š Analyseur de records climatiques
==========================================

ðŸ“‚ Lecture des donnÃ©es historiques depuis HDFS: /hdfs-data
ðŸ“„ 1 fichier(s) historique(s) trouvÃ©(s)
âœ… 3650 jour(s) de donnÃ©es chargÃ©(s)

ðŸ“Š Calcul des records climatiques...

âœ… 1 ville(s) analysÃ©e(s)

ðŸ“‹ AperÃ§u des records:
+-------+-----+--------+---------+-------------+---------------+-------------+---------------+----------------+------------------+
|country|city |latitude|longitude|max_temp_date|max_temperature|min_temp_date|min_temperature|max_wind_date   |max_windspeed     |
+-------+-----+--------+---------+-------------+---------------+-------------+---------------+----------------+------------------+
|France |Paris|48.8566 |2.3522   |2019-07-25   |42.6           |2018-02-28   |-10.2          |2020-12-10      |28.5              |
+-------+-----+--------+---------+-------------+---------------+-------------+---------------+----------------+------------------+

ðŸ’¾ Sauvegarde des records dans HDFS...
âœ… Records sauvegardÃ©s: /hdfs-data/France/Paris/weather_records/records.json

ðŸ“¤ Envoi des records Ã  Kafka: weather_records...
âœ… Record envoyÃ©: Paris, France
âœ… 1 record(s) envoyÃ©(s) Ã  Kafka

==========================================
âœ… Analyse terminÃ©e!
==========================================
```

## VÃ©rification des records

### Dans HDFS

```bash
# Afficher les records pour Paris
docker exec namenode hdfs dfs -cat /hdfs-data/France/Paris/weather_records/records.json | python3 -m json.tool
```

### Dans Kafka

```bash
# Consommer les records
python3 kafka_consumer.py weather_records --from-beginning
```

### Interface Web HDFS

Ouvrir `http://localhost:9870` et naviguer vers `/hdfs-data/{country}/{city}/weather_records/records.json`

## DÃ©pannage

### Erreur : "Aucune donnÃ©e historique disponible"

Assurez-vous d'avoir exÃ©cutÃ© l'exercice 9 pour tÃ©lÃ©charger les donnÃ©es historiques :

```bash
python3 weather_history_loader.py --city Paris --country France --years 10
```

### Erreur : "Aucun fichier historique trouvÃ© dans HDFS"

VÃ©rifiez que les fichiers existent :

```bash
docker exec namenode hdfs dfs -ls -R /hdfs-data | grep weather_history_raw
```

### Erreur lors de l'envoi Ã  Kafka

- VÃ©rifiez que Kafka est accessible
- VÃ©rifiez que le topic `weather_records` existe
- Consultez les logs pour plus de dÃ©tails

### Les dates des records ne sont pas correctes

- VÃ©rifiez que les donnÃ©es historiques contiennent des dates valides
- VÃ©rifiez que les donnÃ©es ne contiennent pas trop de valeurs nulles
- Le script utilise la premiÃ¨re occurrence en cas d'Ã©galitÃ© (ordre par date dÃ©croissante)

## Utilisation des records pour un dashboard

Les records peuvent Ãªtre consommÃ©s depuis Kafka pour alimenter un dashboard :

```python
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'weather_records',
    bootstrap_servers='localhost:29092',
    value_deserializer=lambda x: json.loads(x.decode('utf-8'))
)

for message in consumer:
    record = message.value
    city = record['city']
    country = record['country']
    hottest = record['records']['hottest_day']
    coldest = record['records']['coldest_day']
    
    print(f"{city}, {country}:")
    print(f"  Plus chaud: {hottest['temperature']}Â°C le {hottest['date']}")
    print(f"  Plus froid: {coldest['temperature']}Â°C le {coldest['date']}")
```

## Notes importantes

- **Performance** : Pour de grandes quantitÃ©s de donnÃ©es, le traitement peut prendre plusieurs minutes
- **DonnÃ©es manquantes** : Les jours avec des valeurs nulles sont exclus du calcul
- **Ã‰galitÃ©s** : En cas d'Ã©galitÃ©, la date la plus rÃ©cente est choisie
- **Format** : Les records sont sauvegardÃ©s en JSON dans HDFS et envoyÃ©s en JSON Ã  Kafka

## Exemple de workflow complet

```bash
# 1. TÃ©lÃ©charger les donnÃ©es historiques
python3 weather_history_loader.py --city Paris --country France --years 10

# 2. Analyser les records
./run_records_analyzer.sh

# 3. Visualiser les rÃ©sultats
docker exec namenode hdfs dfs -cat /hdfs-data/France/Paris/weather_records/records.json | python3 -m json.tool
```
